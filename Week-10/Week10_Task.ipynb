{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "720ab022-6835-4613-a13f-456ec7e3adb8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Step1\n",
    "### Create a Dummy Data Generator in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2b01daf-1049-43f5-be1a-94209729bda6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Number: 98\nGenerated Number: 93\nGenerated Number: 72\nGenerated Number: 86\nGenerated Number: 60\nGenerated Number: 60\nGenerated Number: 70\nGenerated Number: 95\nGenerated Number: 51\nGenerated Number: 65\nGenerated Number: 81\nGenerated Number: 62\nGenerated Number: 91\nGenerated Number: 57\nGenerated Number: 69\nGenerated Number: 92\nGenerated Number: 66\nGenerated Number: 69\nGenerated Number: 58\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "def generate_random_numbers():\n",
    "    while True:\n",
    "        number = random.randint(50, 100)\n",
    "        print(f\"Generated Number: {number}\")\n",
    "        time.sleep(2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_random_numbers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "498dfb64-4253-4a06-a8c4-16d31fb18d79",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Step3\n",
    "### Capture the Incoming Event Hub Stream Using Structured Streaming in Databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e217ee01-74b0-4c89-b1dd-9712706610e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- body: binary (nullable = true)\n |-- partition: string (nullable = true)\n |-- offset: string (nullable = true)\n |-- sequenceNumber: long (nullable = true)\n |-- enqueuedTime: timestamp (nullable = true)\n |-- publisher: string (nullable = true)\n |-- partitionKey: string (nullable = true)\n |-- properties: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n |-- systemProperties: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"EventHubStreaming\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define Event Hub connection parameters\n",
    "event_hub_connection_str = \"Endpoint=sb://demoeventhubnew.servicebus.windows.net/;SharedAccessKeyName=coedemo;SharedAccessKey=eGE8FbunCqJa6mrXw1/lARorurbgGgEFe+AEhN5cjqc=;EntityPath=my-event-hub\"\n",
    "event_hub_name = \"my-event-hub\"\n",
    "\n",
    "# Define the Event Hub configurations\n",
    "event_hub_config = {\n",
    "    \"eventhubs.connectionString\": event_hub_connection_str,\n",
    "    \"eventhubs.consumerGroup\": \"$Default\",\n",
    "    \"eventhubs.maxEventsPerTrigger\": \"1000\"\n",
    "}\n",
    "\n",
    "# Read stream from Event Hub\n",
    "df = spark.readStream \\\n",
    "    .format(\"eventhubs\") \\\n",
    "    .options(**event_hub_config) \\\n",
    "    .load()\n",
    "\n",
    "# Display the schema of the incoming data\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ee897b0-57cf-401b-a939-50fb7dacf6a3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Step4\n",
    "### Add a \"Risk\" Column to the Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d32459fa-c50f-4a02-8a39-3f0c2793e37e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"EventHubStream\").getOrCreate()\n",
    "\n",
    "eventHubsConf = {\n",
    "  'eventhubs.connectionString': event_hub_connection_str\n",
    "}\n",
    "\n",
    "# Read stream from Event Hub\n",
    "df = spark.readStream \\\n",
    "  .format(\"eventhubs\") \\\n",
    "  .options(**eventHubsConf) \\\n",
    "  .load()\n",
    "\n",
    "# Convert the data and add the 'Risk' column\n",
    "df = df.withColumn(\"value\", df[\"body\"].cast(StringType()).cast(IntegerType()))\n",
    "df = df.withColumn(\"Risk\", when(col(\"value\") > 80, \"High\").otherwise(\"Low\"))\n",
    "\n",
    "# Write to another Event Hub or a destination like Azure Blob Storage\n",
    "df.writeStream \\\n",
    "  .format(\"eventhubs\") \\\n",
    "  .options(**event_hub_config) \\\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5bd71c9-9cc1-49ca-9ace-b51fd37d35e4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Step5\n",
    "### Capture the Output Stream in a Separate Event Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc2a2985-3639-48fb-b2d4-a42bfa4ed830",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the connection string for the output Event Hub\n",
    "output_event_hub_connection_str = \"Endpoint=sb://<your-output-event-hub-namespace>.servicebus.windows.net/;SharedAccessKeyName=<your-policy-name>;SharedAccessKey=<your-access-key>;EntityPath=<your-output-event-hub-name>\"\n",
    "\n",
    "output_event_hub_config = {\n",
    "    \"eventhubs.connectionString\": output_event_hub_connection_str,\n",
    "    \"eventhubs.partitionKey\": \"partitionKey\",\n",
    "    \"eventhubs.maxEventsPerTrigger\": \"1000\"\n",
    "}\n",
    "\n",
    "# Write the processed stream to the output Event Hub\n",
    "query = processed_df.writeStream \\\n",
    "    .format(\"eventhubs\") \\\n",
    "    .options(**output_event_hub_config) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3d164a7-dbae-43e1-896c-11fc88c533fd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Step6\n",
    "### Connect the Output Stream to Power BI Using Stream Analytics\n",
    "1.Create a Stream Analytics Job in Azure.\n",
    "##### \n",
    "2.Set the input as your output Event Hub. \n",
    "#####  \n",
    "3.Set the output as Power BI: - Authorize Power BI. - Create a new dataset and table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ebfd3f1-3ee2-4ded-8df0-80950f3145d8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Step7\n",
    "### Build a Real-time Dashboard in Power BI\n",
    "1.Open Power BI and connect to the dataset created by your Stream Analytics job.\n",
    "##### \n",
    "2.Create a new report and add a bar chart or a table to visualize the count of High and Low risk values.\n",
    "##### \n",
    "3.Publish the report to your Power BI workspace.\n",
    "##### \n",
    "4.Pin the visuals to a dashboard to create a real-time dashboard.\n",
    "##### \n",
    "5.Write a query to count the number of High and Low risk values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4c6ced7-ff4e-474d-8279-a2852ada5421",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Week10_Task",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
